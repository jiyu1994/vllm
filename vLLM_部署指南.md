# vLLM åœ¨ T4 GPU ä¸Šä»æºç ç¼–è¯‘éƒ¨ç½²æŒ‡å—

## ç¯å¢ƒä¿¡æ¯
- **GPU**: NVIDIA Tesla T4 (15360MiB æ˜¾å­˜)
- **å†…å­˜**: 30GB
- **CUDA ç‰ˆæœ¬**: 12.0 
- **é©±åŠ¨ç‰ˆæœ¬**: 525.105.17
- **ç›®æ ‡æ¨¡å‹**: Llama-3.2-3B-Instruct
- **HuggingFace Token**: `hf_YOUR_TOKEN_HERE` (è¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®token)

## ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ

### 1.1 æ£€æŸ¥Pythonç‰ˆæœ¬
```bash
python --version
python3 --version
```
è¾“å‡ºï¼šPython 3.8.10

### 1.2 æ£€æŸ¥GPUå’ŒCUDAä¿¡æ¯
```bash
nvidia-smi
```

### 1.3 æ£€æŸ¥å†…å­˜ä¿¡æ¯
```bash
free -h
```

## ç¬¬äºŒæ­¥ï¼šå®‰è£… Miniconda

### 2.1 ä¸‹è½½Minicondaå®‰è£…åŒ…
```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
```

### 2.2 å®‰è£…Miniconda
```bash
bash ~/miniconda.sh -b -p $HOME/miniconda3
```

### 2.3 æ¿€æ´»condaå¹¶æ¥å—æœåŠ¡æ¡æ¬¾
```bash
# æ¿€æ´»conda
source $HOME/miniconda3/bin/activate

# æ£€æŸ¥ç‰ˆæœ¬
conda --version

# æ¥å—æœåŠ¡æ¡æ¬¾
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r
```

## ç¬¬ä¸‰æ­¥ï¼šåˆ›å»º conda ç¯å¢ƒ

### 3.1 åˆ›å»ºä¸“ç”¨çš„vLLMç¯å¢ƒ
```bash
# ç¡®ä¿condaå·²æ¿€æ´»
source $HOME/miniconda3/bin/activate

# åˆ›å»ºPython 3.11ç¯å¢ƒï¼ˆvLLMè¦æ±‚Python >= 3.9, < 3.13ï¼‰
conda create -n vllm python=3.11 -y
```

### 3.2 æ¿€æ´»vLLMç¯å¢ƒ
```bash
conda activate vllm

# éªŒè¯Pythonç‰ˆæœ¬
python --version
```
è¾“å‡ºåº”è¯¥æ˜¯ï¼šPython 3.11.13

## ç¬¬å››æ­¥ï¼šå®‰è£…ç¼–è¯‘ä¾èµ–

### 4.1 æ›´æ–°åŸºç¡€å·¥å…·
```bash
# ç¡®ä¿ç¯å¢ƒå·²æ¿€æ´»
source $HOME/miniconda3/bin/activate
conda activate vllm

# æ›´æ–°pipå’ŒåŸºç¡€å·¥å…·
pip install -U pip setuptools wheel
```

### 4.2 å®‰è£…ç¼–è¯‘ä¾èµ–åŒ…
```bash
# å®‰è£…ç¼–è¯‘æ‰€éœ€çš„ä¾èµ–ï¼ˆè¿™ä¸€æ­¥ä¼šä¸‹è½½å¤§é‡CUDAåº“ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
pip install -r requirements/build.txt
```

**æ³¨æ„**: è¿™ä¸€æ­¥ä¼šä¸‹è½½çº¦2-3GBçš„CUDAç›¸å…³åº“ï¼ŒåŒ…æ‹¬ï¼š
- PyTorch 2.7.1 (çº¦821MB)
- CUDAè¿è¡Œæ—¶åº“å’Œå·¥å…·
- cuDNNã€cuBLASç­‰CUDAåŠ é€Ÿåº“
- å¤§çº¦éœ€è¦10-15åˆ†é’Ÿå®Œæˆ

## ç¬¬äº”æ­¥ï¼šä»æºç ç¼–è¯‘vLLMï¼ˆä½ éœ€è¦è‡ªå·±æ‰§è¡Œï¼‰

### 5.1 å‡†å¤‡ç¼–è¯‘ç¯å¢ƒ
```bash
# ç¡®ä¿åœ¨vllmæºç ç›®å½•
cd /home/ubuntu/vllm

# æ¿€æ´»ç¯å¢ƒ
source $HOME/miniconda3/bin/activate
conda activate vllm
```

### 5.2 å®‰è£…è¿è¡Œæ—¶ä¾èµ–
```bash
# å®‰è£…CUDAç›¸å…³ä¾èµ–
pip install -r requirements/cuda.txt

# å®‰è£…é€šç”¨ä¾èµ–
pip install -r requirements/common.txt
```

### 5.3 å¼€å§‹ç¼–è¯‘å®‰è£…ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
```bash
# æ–¹æ³•1ï¼šä½¿ç”¨pipå®‰è£…ï¼ˆæ¨èï¼Œä¼šè‡ªåŠ¨å¤„ç†ç¼–è¯‘ï¼‰
pip install -e .

# æˆ–è€…æ–¹æ³•2ï¼šä½¿ç”¨setup.py
python setup.py develop
```

**ç¼–è¯‘è¯´æ˜**ï¼š
- ç¼–è¯‘è¿‡ç¨‹éœ€è¦15-30åˆ†é’Ÿï¼Œå–å†³äºæœºå™¨æ€§èƒ½
- ç¼–è¯‘ä¼šäº§ç”Ÿå¤§é‡è¾“å‡ºï¼Œè¿™æ˜¯æ­£å¸¸çš„
- ä¸»è¦ç¼–è¯‘çš„ç»„ä»¶åŒ…æ‹¬ï¼šCUDA kernelsã€C++æ‰©å±•ã€Pythonç»‘å®šç­‰
- å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œæ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§å’Œå†…å­˜æ˜¯å¦è¶³å¤Ÿ

### 5.4 éªŒè¯å®‰è£…
```bash
# æµ‹è¯•å¯¼å…¥
python -c "import vllm; print('vLLMå®‰è£…æˆåŠŸ!')"

# æ£€æŸ¥ç‰ˆæœ¬
python -c "import vllm; print(f'vLLMç‰ˆæœ¬: {vllm.__version__}')"
```

## ç¬¬å…­æ­¥ï¼šé…ç½®Hugging Face Token

### 6.1 è®¾ç½®ç¯å¢ƒå˜é‡
```bash
# ä¸´æ—¶è®¾ç½®ï¼ˆå½“å‰ä¼šè¯æœ‰æ•ˆï¼‰
export HUGGING_FACE_HUB_TOKEN="hf_YOUR_TOKEN_HERE"

# æˆ–è€…æ°¸ä¹…è®¾ç½®ï¼ˆæ·»åŠ åˆ°bashrcï¼‰
echo 'export HUGGING_FACE_HUB_TOKEN="hf_YOUR_TOKEN_HERE"' >> ~/.bashrc
source ~/.bashrc
```

### 6.2 ä½¿ç”¨huggingface-cliç™»å½•
```bash
# å®‰è£…huggingface hub
pip install huggingface-hub

# ç™»å½•
huggingface-cli login --token hf_YOUR_TOKEN_HERE
```

## ç¬¬ä¸ƒæ­¥ï¼šæµ‹è¯•vLLMå®‰è£…

### 7.1 ç®€å•åŠŸèƒ½æµ‹è¯•
```bash
python -c "
from vllm import LLM, SamplingParams
print('vLLMå¯¼å…¥æˆåŠŸ!')
print('GPUå¯ç”¨æ€§æ£€æŸ¥...')
import torch
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    print(f'GPUå‹å·: {torch.cuda.get_device_name(0)}')
"
```

## ç¬¬å…«æ­¥ï¼šéƒ¨ç½²Llama-3.2-3B-Instructæ¨¡å‹

### 8.1 æµ‹è¯•æ¨¡å‹åŠ è½½
```bash
python -c "
from vllm import LLM, SamplingParams

# åˆ›å»ºLLMå®ä¾‹
llm = LLM(
    model='meta-llama/Llama-3.2-3B-Instruct',
    tensor_parallel_size=1,  # T4æ˜¯å•GPU
    gpu_memory_utilization=0.8,  # ä½¿ç”¨80%æ˜¾å­˜
    max_model_len=2048  # æ ¹æ®T4æ˜¾å­˜é™åˆ¶è®¾ç½®è¾ƒå°çš„ä¸Šä¸‹æ–‡é•¿åº¦
)

print('æ¨¡å‹åŠ è½½æˆåŠŸ!')
"
```

### 8.2 è¿è¡Œæ¨ç†æµ‹è¯•
```bash
python -c "
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model='meta-llama/Llama-3.2-3B-Instruct',
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8,
    max_model_len=2048
)

# åˆ›å»ºé‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=100
)

# æµ‹è¯•æ¨ç†
prompts = ['Hello, how are you?', 'What is machine learning?']
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f'Prompt: {prompt}')
    print(f'Generated: {generated_text}')
    print('-' * 50)
"
```

### 8.3 å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰
```bash
# å¯åŠ¨OpenAIå…¼å®¹çš„APIæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-3B-Instruct \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 2048 \
    --host 0.0.0.0 \
    --port 8000
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

1. **CUDAç‰ˆæœ¬ä¸åŒ¹é…**
   ```bash
   # æ£€æŸ¥CUDAç‰ˆæœ¬
   nvcc --version
   nvidia-smi
   # ç¡®ä¿PyTorch CUDAç‰ˆæœ¬åŒ¹é…
   ```

2. **æ˜¾å­˜ä¸è¶³**
   ```bash
   # é™ä½gpu_memory_utilizationå‚æ•°
   # å‡å°max_model_lenå‚æ•°
   # ä½¿ç”¨é‡åŒ–æ¨¡å‹
   ```

3. **ç¼–è¯‘å¤±è´¥**
   ```bash
   # æ¸…ç†ç¼“å­˜é‡æ–°ç¼–è¯‘
   pip cache purge
   rm -rf build/
   pip install -e . --force-reinstall --no-cache-dir
   ```

4. **Pythonç‰ˆæœ¬ä¸å…¼å®¹**
   ```bash
   # vLLMè¦æ±‚Python 3.9-3.12
   python --version
   # å¦‚éœ€è¦ï¼Œé‡æ–°åˆ›å»ºcondaç¯å¢ƒ
   ```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **T4 GPUä¼˜åŒ–å‚æ•°**ï¼š
   - `gpu_memory_utilization=0.8`ï¼ˆä¸è¦è®¾ç½®å¤ªé«˜ï¼‰
   - `max_model_len=2048`ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
   - `tensor_parallel_size=1`ï¼ˆå•GPUï¼‰

2. **æ¨¡å‹é€‰æ‹©**ï¼š
   - 3Bå‚æ•°çš„æ¨¡å‹é€‚åˆT4
   - å¯ä»¥è€ƒè™‘ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬èŠ‚çœæ˜¾å­˜

3. **æ‰¹å¤„ç†**ï¼š
   - é€‚å½“è®¾ç½®batch sizeä»¥æé«˜ååé‡

## éƒ¨ç½²çŠ¶æ€æ€»ç»“

âœ… **æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼**

- [x] ç¯å¢ƒæ£€æŸ¥å’Œå‡†å¤‡
- [x] Minicondaå®‰è£…å’Œé…ç½®  
- [x] condaè™šæ‹Ÿç¯å¢ƒåˆ›å»º
- [x] ç¼–è¯‘ä¾èµ–å®‰è£…
- [x] vLLMä»æºç ç¼–è¯‘æˆåŠŸ
- [x] Hugging Face tokené…ç½®
- [x] vLLMåŠŸèƒ½æµ‹è¯•æˆåŠŸ

**å®‰è£…çš„vLLMç‰ˆæœ¬**: `0.1.dev8596+g74f441f4b.cu120`

## å…³äºLlamaæ¨¡å‹è®¿é—®

ä½ æä¾›çš„tokenæ²¡æœ‰è®¿é—®Meta Llamaæ¨¡å‹çš„æƒé™ã€‚Llamaæ¨¡å‹éœ€è¦ï¼š
1. å…ˆåœ¨Hugging Faceä¸Šç”³è¯·è®¿é—®æƒé™ï¼šhttps://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
2. Metaä¼šå®¡æ ¸ç”³è¯·ï¼ˆé€šå¸¸å‡ å°æ—¶åˆ°å‡ å¤©ï¼‰
3. è·å¾—æƒé™åå°±å¯ä»¥ä½¿ç”¨ä½ çš„tokenè®¿é—®

## æ›¿ä»£æ–¹æ¡ˆ

å¯ä»¥ä½¿ç”¨å…¶ä»–ä¼˜ç§€çš„å¼€æºæ¨¡å‹ï¼š
- **Qwen/Qwen2.5-3B-Instruct** - é˜¿é‡Œå·´å·´çš„é«˜è´¨é‡ä¸­æ–‡æ¨¡å‹
- **microsoft/DialoGPT-medium** - å¾®è½¯å¯¹è¯æ¨¡å‹
- **google/flan-t5-base** - Googleçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹

## å¯åŠ¨APIæœåŠ¡ç¤ºä¾‹

```bash
# æ¿€æ´»ç¯å¢ƒ
source $HOME/miniconda3/bin/activate
conda activate vllm
export HUGGING_FACE_HUB_TOKEN="hf_YOUR_TOKEN_HERE"

# å¯åŠ¨Qwenæ¨¡å‹APIæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-3B-Instruct \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 2048 \
    --host 0.0.0.0 \
    --port 8000

# æµ‹è¯•API
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-3B-Instruct",
        "prompt": "Hello, how are you?",
        "max_tokens": 50
    }'
```

## ä¸‹ä¸€æ­¥å»ºè®®

1. **ç”³è¯·Llamaè®¿é—®æƒé™**ï¼ˆå¦‚æœéœ€è¦ï¼‰
2. **æµ‹è¯•ä¸åŒæ¨¡å‹**ï¼Œæ‰¾åˆ°æœ€é€‚åˆä½ åº”ç”¨çš„
3. **éƒ¨ç½²APIæœåŠ¡**ï¼Œé›†æˆåˆ°ä½ çš„åº”ç”¨ä¸­
4. **ä¼˜åŒ–å‚æ•°**ï¼Œæ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´æ€§èƒ½è®¾ç½®

---

**ğŸ‰ æ­å–œï¼vLLMæ¨ç†æ¡†æ¶å·²åœ¨ä½ çš„T4æœºå™¨ä¸ŠæˆåŠŸéƒ¨ç½²ï¼**